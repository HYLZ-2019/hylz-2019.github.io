<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://hylz-2019.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://hylz-2019.github.io/" rel="alternate" type="text/html" hreflang="en" /><updated>2025-08-25T07:40:47+00:00</updated><id>https://hylz-2019.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
</subtitle><entry><title type="html">Map of Neuromorphic Vision Institutes</title><link href="https://hylz-2019.github.io/blog/2025/world-map/" rel="alternate" type="text/html" title="Map of Neuromorphic Vision Institutes" /><published>2025-08-25T00:00:00+00:00</published><updated>2025-08-25T00:00:00+00:00</updated><id>https://hylz-2019.github.io/blog/2025/world-map</id><content type="html" xml:base="https://hylz-2019.github.io/blog/2025/world-map/"><![CDATA[<h2 id="abstract">Abstract</h2>

<p>This project visualizes the collaborative relationships between institutions engaged in neuromorphic vision research. By analyzing co-authorship patterns from academic papers, we create an interactive world map that shows the global distribution of neuromorphic vision research institutions and their collaboration networks. The visualization reveals the interconnected nature of this emerging field and highlights major research hubs worldwide.</p>

<div class="text-center mb-4">
    <a href="/assets/html/Map_Of_Neuromorphic_Vision_Institutes_20250825.html" class="btn btn-primary btn-lg" target="_blank">
        üó∫Ô∏è View Interactive Map
    </a>
</div>

<p><br /></p>

<h2 id="project-overview">Project Overview</h2>

<p>The neuromorphic vision field has seen tremendous growth in recent years, with researchers worldwide contributing to advances in event-based cameras, spiking neural networks, and bio-inspired vision algorithms. This visualization project aims to map the global landscape of neuromorphic vision research by analyzing institutional collaborations through academic publications.</p>

<p>The interactive map displays:</p>
<ul>
  <li><strong>Geographic distribution</strong> of research institutions</li>
  <li><strong>Collaboration networks</strong> based on co-authorship patterns</li>
  <li><strong>Institution details</strong> and research focus areas</li>
  <li><strong>Publication statistics</strong> for each institution</li>
</ul>

<p><br /></p>

<h2 id="data-sources-and-methodology">Data Sources and Methodology</h2>

<h3 id="data-collection">Data Collection</h3>
<p>The paper list used in this project is sourced from a comprehensive <a href="https://docs.google.com/spreadsheets/d/1_OBbSz10CkxXNDHQd-Mn_ui3OmymMFvm-lW316uvxy8/edit?gid=0#gid=0">Google Sheets database</a>, with data collected up to August 19, 2025. Based on the paper titles and conference/journal information in the list, I used AI agents to query author lists through arXiv and Semantic Scholar APIs.</p>

<h3 id="institution-extraction">Institution Extraction</h3>
<p>For papers available on arXiv, I employed AI agents to extract and clean institutional information from their front pages, followed by manual verification and additional cleaning work. The geographic coordinates used to visualize institutional locations are provided by large language models.</p>

<h3 id="data-limitations">Data Limitations</h3>
<p>Due to technical limitations, information for many papers was not successfully extracted, and many authors‚Äô institutions remain unannotated (categorized as ‚Äúunknown‚Äù). The positions on the map are not precise and are adjusted using a simple algorithm to spread out overly dense points for better visualization.</p>

<h3 id="technical-implementation">Technical Implementation</h3>
<p>The visualization webpage code was primarily written by Claude 4-based Copilot under my guidance through several iterations, combining modern web technologies for interactive data visualization.</p>

<p><br /></p>

<h2 id="acknowledgments">Acknowledgments</h2>

<p>Thank you to all neuromorphic vision practitioners for their contributions to this rapidly evolving field. I hope we can work together to make the industry flourish and make the world better!</p>

<p>Special thanks to the research community for maintaining open access to publication data and enabling collaborative research visualization projects like this one.</p>

<hr />

<p><strong>Author:</strong> HYLZ (<a href="https://github.com/HYLZ-2019">GitHub</a>)<br />
<strong>Last Updated:</strong> August 25, 2025</p>]]></content><author><name></name></author><category term="neuromorphic vision" /><category term="collaboration" /><category term="visualization" /><category term="interactive map" /><summary type="html"><![CDATA[A visualization of collaborative relationships between institutions engaged in neuromorphic vision research.]]></summary></entry><entry><title type="html">All-in-focus Imaging from Event Focal Stack</title><link href="https://hylz-2019.github.io/blog/2023/aif-efs/" rel="alternate" type="text/html" title="All-in-focus Imaging from Event Focal Stack" /><published>2023-03-21T00:00:00+00:00</published><updated>2023-03-21T00:00:00+00:00</updated><id>https://hylz-2019.github.io/blog/2023/aif-efs</id><content type="html" xml:base="https://hylz-2019.github.io/blog/2023/aif-efs/"><![CDATA[<h2 id="abstract">Abstract</h2>
<p>Traditional focal stack methods require multiple shots to capture images focused at different distances of the same scene, which cannot be applied to dynamic scenes well. Generating a high-quality all-in-focus image from a single shot is challenging, due to the highly ill-posed nature of the single-image defocus and deblurring problem. In this thesis, to restore an all-in-focus image, we propose the event focal stack which is defined as event streams captured during a continuous focal sweep. Given an RGB image focused at an arbitrary distance, we explore the high temporal resolution of event streams, from which we automatically select refocusing timestamps and reconstruct corresponding refocused images with events to form a focal stack. Guided by the neighbouring events around the selected timestamps, we can merge the focal stack with proper weights and restore a sharp all-in-focus image. Experimental results on both synthetic and real datasets show superior performance over state-of-the-art methods.</p>

<div>
  <div style="position:relative;padding-top:56.25%;">
    <iframe src="https://www.youtube.com/embed/9HQLqj4cY7o" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="" style="position:absolute;top:0;left:0;width:100%;height:100%;"></iframe>
  </div>
</div>

<p><br /></p>

<h2 id="animated-results">Animated Results</h2>
<p>Visual quality comparison with an image-based focal stack method.</p>
<div style="display:flex;flex-direction:row;justify-content:center;align-items:flex-start;">
    <div class="image-with-caption" style="margin-right:1em">
        <figure>

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/efs/shu_images.gif" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

        <p>(a) Image focal stack.</p>
    </div>
    <div class="image-with-caption" style="margin-right:1em">
        <figure>

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/efs/imageStackRes.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

        <p>(b) All-in-focus image restored by Zhou et al.</p>
    </div>
    <div class="image-with-caption" style="margin-right:1em">
        <figure>

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/efs/shu_events.gif" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

        <p>(c) The visualization of EFS.</p>
    </div>
    <div class="image-with-caption">
        <figure>

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/efs/eventStackRes.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

        <p>(d) All-in-focus image restored by our method.</p>
    </div>
</div>

<p><br /></p>
<h2 id="bibtex">BibTex</h2>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@InProceedings{Lou_2023_CVPR,
    author    = {Lou, Hanyue and Teng, Minggui and Yang, Yixin and Shi, Boxin},
    title     = {All-in-Focus Imaging From Event Focal Stack},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2023},
    pages     = {17366-17375}
}
</code></pre></div></div>]]></content><author><name></name></author><summary type="html"><![CDATA[Hanyue Lou*, Minggui Teng*, Yixin Yang, and Boxin Shi. CVPR 2023.]]></summary></entry></feed>