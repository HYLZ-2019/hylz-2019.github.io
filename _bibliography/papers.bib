---
---
@inproceedings{Lou2024zest,
  title={Zero-Shot Event-Intensity Asymmetric Stereo via Visual Prompting from Image Domain},
  author={Lou, Hanyue and Liang, Jinxiu and Teng, Minggui and Fan, Bin and Xu, Yong and Shi, Boxin},
  booktitle={Advances in Neural Information Processing Systems},
  year={2024},
  preview={zest-thumbnail.jpg},
  selected={true},
  abstract={Event-intensity asymmetric stereo systems have emerged as a promising approach for robust 3D perception in dynamic and challenging environments by integrating event cameras with frame-based sensors in different views. However, existing methods often suffer from overfitting and poor generalization due to limited dataset sizes and lack of scene diversity in the event domain. To address these issues, we propose a zero-shot framework that utilizes monocular depth estimation and stereo matching models pretrained on diverse image datasets. Our approach introduces a visual prompting technique to align the representations of frames and events, allowing the use of off-the-shelf stereo models without additional training. Furthermore, we introduce a monocular cue-guided disparity refinement module to improve robustness across static and dynamic regions by incorporating monocular depth information from foundation models. Extensive experiments on real-world datasets demonstrate the superior zero-shot evaluation performance and enhanced generalization ability of our method compared to existing approaches.},
  url={https://neurips.cc/virtual/2024/poster/96057},
  pdf={https://assets.ctfassets.net/yreyglvi5sud/5IrCvcOrMCeh9n1vvwAoyV/9077437a69e373cce42670a5d3add0b0/Lou_NeurIPS24.pdf},
  code={https://github.com/HYLZ-2019/ZEST},
}

@inproceedings{Lou2023allinfocus,
  title={All-in-focus Imaging from Event Focal Stack},
  author={Lou, Hanyue and Teng, Minggui and Yang, Yixin and Shi, Boxin},
  booktitle={Proc. of Conference on Computer Vision and Pattern Recognition},
  year={2023},
  preview={efs-thumbnail.png},
  selected={true},
  abstract={Traditional focal stack methods require multiple shots to capture images focused at different distances of the same scene, which cannot be applied to dynamic scenes well. Generating a high-quality all-in-focus image from a single shot is challenging, due to the highly ill-posed nature of the single-image defocus and deblurring problem. In this thesis, to restore an all-in-focus image, we propose the event focal stack which is defined as event streams captured during a continuous focal sweep. Given an RGB image focused at an arbitrary distance, we explore the high temporal resolution of event streams, from which we automatically select refocusing timestamps and reconstruct corresponding refocused images with events to form a focal stack. Guided by the neighbouring events around the selected timestamps, we can merge the focal stack with proper weights and restore a sharp all-in-focus image. Experimental results on both synthetic and real datasets show superior performance over state-of-the-art methods. },
  url={https://hylz-2019.github.io/EFS},
  pdf={https://downloads.ctfassets.net/yreyglvi5sud/5cpPeCBHgnzMWHztPOipt1/d342ddfbf1c153edcc69dc128ca5de14/Lou_CVPR23b.pdf},
  code={https://github.com/HYLZ-2019/EFS},
}

@inproceedings{teng2022nest,
  title = {NEST: Neural Event Stack for Event-based Image Enhancement},
  author={Teng, Minggui and Zhou, Chu and Lou, Hanyue and Shi, Boxin},
  booktitle = {Proc. of European Conference on Computer Vision},
  pages={660--676},
  year = {2022},
  organization={Springer},
  preview={nest-thumbnail.png},
  selected={true},
  abstract={Event cameras demonstrate unique characteristics such as high temporal resolution, low latency, and high dynamic range to improve performance for various image enhancement tasks. However, event streams cannot be applied to neural networks directly due to their sparse nature. To integrate events into traditional computer vision algorithms, an appropriate event representation is desirable, while existing voxel grid and event stack representations are less effective in encoding motion and temporal information. This paper presents a novel event representation named Neural Event STack (NEST), which satisfies physical constraints and encodes comprehensive motion and temporal information sufficient for image enhancement. We apply our representation on multiple tasks, which achieves superior performance on image deblurring and image super-resolution than state-of-the-art methods on both synthetic and real datasets. And we further demonstrate the possibility to generate high frame rate videos with our novel event representation.},
  url={https://tengminggui.cn/publication/eccv22/},
  pdf={https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136660649.pdf},
  code={https://github.com/ChipsAhoyM/NEST},
}
